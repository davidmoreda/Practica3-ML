{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb0d0641",
   "metadata": {},
   "source": [
    "# Clase `myLogisticRegression`\n",
    "\n",
    "La clase implementa un modelo de **regresión logística multiclase**, siguiendo exactamente la *generalización multinomial One-vs-All implícita* presentada en la teoría.\n",
    "\n",
    "El objetivo es estimar la probabilidad de que una observación pertenezca a una de las \\(J\\) clases posibles, modelando esta probabilidad mediante funciones logísticas y optimizando los parámetros con **descenso del gradiente** sobre la entropía cruzada multiclase.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Modelo Multiclase (One-vs-All implícito)\n",
    "\n",
    "A diferencia del modelo binario, donde solo existe un vector de pesos \\(w\\), en el caso multiclase se define un vector para cada clase excepto la última:\n",
    "\n",
    "$$\n",
    "w_1, w_2, \\dots, w_{J-1}\n",
    "$$\n",
    "\n",
    "Cada vector modela la contribución lineal:\n",
    "\n",
    "$$\n",
    "z_{ij} = w_j^\\top x_i\n",
    "$$\n",
    "\n",
    "donde \\(x_i\\) incluye un primer componente igual a 1 (bias).\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Probabilidades de clase\n",
    "\n",
    "La generalización del Tema 6 utiliza el siguiente esquema:\n",
    "\n",
    "### Para las clases \\(1,\\dots,J-1\\):\n",
    "\n",
    "$$\n",
    "p_{ij} = \n",
    "\\frac{\\exp(w_j^\\top x_i)}\n",
    "{1 + \\sum_{m=1}^{J-1}\\exp(w_m^\\top x_i)}\n",
    "$$\n",
    "\n",
    "### Para la última clase \\(J\\):\n",
    "\n",
    "$$\n",
    "p_{iJ} = 1 - \\sum_{j=1}^{J-1} p_{ij}\n",
    "$$\n",
    "\n",
    "Esta formulación garantiza que:\n",
    "\n",
    "$$\n",
    "\\sum_{j=1}^{J} p_{ij} = 1\n",
    "$$\n",
    "\n",
    "y evita tener que definir un vector de pesos explícito para la última clase.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Función de pérdida: entropía cruzada multiclase\n",
    "\n",
    "Sea \\(Y\\) la matriz *one-hot* donde:\n",
    "\n",
    "- \\(Y_{ij} = 1\\) si la muestra \\(i\\) pertenece a la clase \\(j\\)\n",
    "- \\(0\\) en caso contrario\n",
    "\n",
    "La pérdida es:\n",
    "\n",
    "$$\n",
    "E = -\\frac{1}{N}\\sum_{i=1}^N \\sum_{j=1}^J Y_{ij} \\log(p_{ij})\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Gradiente\n",
    "\n",
    "El gradiente respecto a cada vector de pesos \\(w_j\\), para \\(j = 1,\\dots,J-1\\), viene dado por:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial w_j}\n",
    "=\n",
    "\\frac{1}{N}\n",
    "\\sum_{i=1}^N (p_{ij} - Y_{ij}) x_i\n",
    "$$\n",
    "\n",
    "De esta forma, todos los vectores se actualizan simultáneamente en cada iteración.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Regla de actualización\n",
    "\n",
    "Usando descenso del gradiente con tasa de aprendizaje \\(\\eta\\):\n",
    "\n",
    "$$\n",
    "w_j \\leftarrow w_j - \\eta \\, \\frac{\\partial E}{\\partial w_j}\n",
    "$$\n",
    "\n",
    "Solo se actualizan los pesos de las primeras \\(J-1\\) clases, ya que la clase restante está implícita.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Predicción\n",
    "\n",
    "Para un nuevo vector \\(x\\):\n",
    "\n",
    "1. Se calculan las probabilidades generalizadas \\(p_{ij}\\)\n",
    "2. Se selecciona la clase con mayor probabilidad:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\arg\\max_j \\, p_{ij}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Resumen de características del clasificador\n",
    "\n",
    "* Modelo multinomial con \\(J-1\\) vectores de pesos.\n",
    "* Probabilidades consistentes con la formulación del Tema 6.\n",
    "* Optimización mediante **descenso del gradiente batch**.\n",
    "* Compatible con scikit-learn (hereda de `BaseEstimator` y `ClassifierMixin`).\n",
    "* Implementación totalmente vectorizada.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56d8e7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "class myLogisticRegression(BaseEstimator, ClassifierMixin):\n",
    "\n",
    "    def __init__(self, learning_rate=0.01, max_iter=1000, tol=1e-6):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.W_ = None\n",
    "        self.classes_ = None\n",
    "\n",
    "    def _softmax_generalized(self, Z):\n",
    "        \"\"\"\n",
    "        Z tiene dimensión (N, J-1).\n",
    "        Implementa EXACTAMENTE la generalización del Tema 6:\n",
    "\n",
    "            p_j = exp(z_j) / (1 + sum_m exp(z_m))   para j = 1..J-1\n",
    "            p_J = 1 - sum_j p_j\n",
    "\n",
    "        Devuelve probas de dimensión (N, J).\n",
    "        \"\"\"\n",
    "        expZ = np.exp(Z)\n",
    "        denom = 1 + np.sum(expZ, axis=1, keepdims=True)\n",
    "        P_small = expZ / denom               # p1..p(J-1)\n",
    "        p_last = 1 - np.sum(P_small, axis=1, keepdims=True)\n",
    "        return np.hstack([P_small, p_last])\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y).ravel()\n",
    "\n",
    "        N, k = X.shape\n",
    "        self.classes_ = np.unique(y)\n",
    "        J = len(self.classes_)\n",
    "\n",
    "        # Matriz X con bias\n",
    "        Xb = np.hstack([np.ones((N, 1)), X])   # (N, k+1)\n",
    "\n",
    "        # ONE-HOT de y → matriz Y (N, J)\n",
    "        Y = np.zeros((N, J))\n",
    "        for idx, c in enumerate(self.classes_):\n",
    "            Y[y == c, idx] = 1\n",
    "\n",
    "        # W tendrá dimensión (k+1, J-1)\n",
    "        rng = np.random.default_rng()\n",
    "        self.W_ = rng.uniform(-0.01, 0.01, size=(k+1, J-1))\n",
    "\n",
    "        prev_loss = np.inf\n",
    "\n",
    "        for _ in range(self.max_iter):\n",
    "\n",
    "            # Forward → Z = XW (N × (J-1))\n",
    "            Z = Xb.dot(self.W_)\n",
    "\n",
    "            # Probabilidades generalizadas (Tema 6, pág. 54)\n",
    "            P = self._softmax_generalized(Z)  # (N, J)\n",
    "\n",
    "            # Cross-entropy multiclase\n",
    "            loss = -np.mean(np.sum(Y * np.log(P + 1e-12), axis=1))\n",
    "\n",
    "            # Gradiente: dE/dW_j para j=1..J-1\n",
    "            # Solo actualizamos hasta J-1 (la última clase es implícita)\n",
    "            error = (P - Y)[:, :J-1]           # (N, J-1)\n",
    "            grad = Xb.T.dot(error) / N        # (k+1, J-1)\n",
    "\n",
    "            # Descenso del gradiente\n",
    "            self.W_ -= self.learning_rate * grad\n",
    "\n",
    "            # Criterio de parada\n",
    "            if abs(prev_loss - loss) < self.tol:\n",
    "                break\n",
    "            prev_loss = loss\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.asarray(X)\n",
    "        Xb = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "\n",
    "        Z = Xb.dot(self.W_)                  # (N, J-1)\n",
    "        P = self._softmax_generalized(Z)     # (N, J)\n",
    "\n",
    "        preds_idx = np.argmax(P, axis=1)\n",
    "        return self.classes_[preds_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92de9335",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data/data_clasificacion.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d28b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4629917759502112\n",
      "F1 weighted: 0.4246846544191894\n",
      "Matriz de confusión:\n",
      " [[ 96   0   0  10  13   0   0  19   0  18   0]\n",
      " [  0   0  17   0   0  20  39   8  23  46 190]\n",
      " [  0   0  71   0   0   6  31   3   8  57 142]\n",
      " [ 15   0   0  68   5   1   0   4   0   7   0]\n",
      " [ 25   0   0   5  45   0   0   2   0  20   0]\n",
      " [  0   0   8   0   0 243  15   0   4  62  30]\n",
      " [  1   0  23   4   0  44  73  11  45 121 325]\n",
      " [ 16   0   0   5   0   0   0 122   0   1   0]\n",
      " [  0   0   2   0   0   1  13   0 253   1 194]\n",
      " [ 27   0  25   5   8  43  40   1   5 299 178]\n",
      " [ 10   0  71   6  22  16  61   2 142  94 813]]\n"
     ]
    }
   ],
   "source": [
    "X = df.drop(columns=[\"Class\"]).values\n",
    "y = df[\"Class\"].values\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "model = myLogisticRegression(\n",
    "    learning_rate=0.05,\n",
    "    max_iter=5000,\n",
    "    tol=1e-6\n",
    ")\n",
    "\n",
    "model.fit(X_train_scaled, y_train)\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"F1 weighted:\", f1_score(y_test, y_pred, average='weighted'))\n",
    "print(\"Matriz de confusión:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (master-ai)",
   "language": "python",
   "name": "master-ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
