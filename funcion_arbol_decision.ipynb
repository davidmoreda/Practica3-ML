{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "808c74f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be928ef9",
   "metadata": {},
   "source": [
    "## $\\texttt{myDecisionTreeClassifier}$\n",
    "\n",
    "El modelo soporta dos criterios de partición:\n",
    "- $\\textbf{ID3}$: utiliza la entropía y la ganancia de información.\n",
    "- $\\textbf{Gini}$: utiliza el índice de Gini y la reducción de impureza.\n",
    "\n",
    "El constructor del modelo:\n",
    "\n",
    "```python\n",
    "MyDecisionTreeClassifier(criterio=\"gini\", \n",
    "                         max_depth=None, \n",
    "                         min_samples_split=2)\n",
    "```\n",
    "\n",
    "Define los siguientes hiperparámetros:\n",
    "\n",
    "- `criterio`: puede ser \\texttt{\"id3\"} o \\texttt{\"gini\"}.  \n",
    "    - Si es $\\texttt{\"id3\"}$, el modelo usa entropía y ganancia de información\n",
    "        como en ID3.\n",
    "    - Si es $\\texttt{\"gini\"}$, utiliza el índice Gini característico de CART.\n",
    "\n",
    "- `max_depth`: profundización máxima del árbol, \n",
    "    implementando pre-poda.\n",
    "    \n",
    "- `min_samples_split`: tamaño mínimo de un nodo para poder dividirse.\n",
    "    \n",
    "\n",
    "\n",
    "En ID3 original no existe poda, pero el temario recomienda técnicas \n",
    "de pre-poda para evitar sobreajuste.\n",
    "\n",
    "---\n",
    "### Criterios\n",
    "\n",
    "#### Entropía (ID3)\n",
    "\n",
    "La función:\n",
    "\n",
    "```python\n",
    "def entropy(self, y):\n",
    "    ...\n",
    "```\n",
    "\n",
    "implementa exactamente la definición teórica:\n",
    "\n",
    "$$\n",
    "H(C) = -\\sum_{i=1}^{K} p(C_i)\\,\\log_2 p(C_i),\n",
    "$$\n",
    "Esto quiere decir:\n",
    "- $H(C)=0$ si el nodo es puro,\n",
    "- $H(C)$ es máxima cuando las clases están uniformemente distribuidas.\n",
    "\n",
    "#### Gini\n",
    "\n",
    "La función:\n",
    "\n",
    "```python\n",
    "def gini(self, y):\n",
    "    ...\n",
    "```\n",
    "\n",
    "implementa la fórmula teórica del índice Gini:\n",
    "\n",
    "$$\n",
    "Gini(T) = 1 - \\sum_{i=1}^{K} p_i^2,\n",
    "$$\n",
    "\n",
    "El índice Gini es cero cuando todas las instancias pertenecen a la misma clase.\n",
    "\n",
    "---\n",
    "\n",
    "### Ganancia de Información / Reducción de Impureza\n",
    "\n",
    "La función:\n",
    "\n",
    "```python\n",
    "information_gain(self, y, y_left, y_right, impurity_func)\n",
    "```\n",
    "calcula la mejora de impureza producida al dividir un nodo en dos subconjuntos.\n",
    "\n",
    "Sea $T$ el nodo padre y $T_L$, $T_R$ los nodos hijos.\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{Gain} &= \n",
    "\\text{Impureza}(T) \\;-\\;\n",
    "\\left(\n",
    "    \\frac{|T_L|}{|T|} \\text{Impureza}(T_L)\n",
    "    +\n",
    "    \\frac{|T_R|}{|T|} \\text{Impureza}(T_R)\n",
    "\\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "- Con entropía, esto coincide exactamente con la $\\textbf{Ganancia de Información}$:\n",
    "    $$\n",
    "    I(C,X) = H(C) - H(C \\mid X)\n",
    "    $$\n",
    "- Con Gini, coincide con la $\\textbf{reducción de impureza de CART}$.\n",
    "\n",
    "---\n",
    "### Selección del Mejor Punto de Corte\n",
    "\n",
    "La función:\n",
    "\n",
    "```python\n",
    "best_split(self, X, y)\n",
    "```\n",
    "\n",
    "evalúa todas las características y todos los posibles umbrales para encontrar la partición que maximiza la ganancia.\n",
    "\n",
    "\n",
    "1. Ordenar los valores del atributo.\n",
    "    \\item Calcular puntos medios entre valores consecutivos:\n",
    "    $$\n",
    "        t_j = \\frac{v_j + v_{j+1}}{2}.\n",
    "    $$\n",
    "2. Evaluar los splits binarios:\n",
    "    $$\n",
    "        X_i < t_j \\qquad \\text{y} \\qquad X_i \\ge t_j.\n",
    "    $$\n",
    "3.  Seleccionar el umbral que maximiza la reducción de impureza.\n",
    "\n",
    "\n",
    "\n",
    "La función devuelve:\n",
    "- el índice de la mejor característica;\n",
    "- el mejor umbral encontrado.\n",
    "\n",
    "Si ningún split mejora la impureza, se devuelve $\\texttt{None}$ y el nodo se convertirá en hoja.\n",
    "\n",
    "---\n",
    "\n",
    "### Construcción Recursiva del Árbol\n",
    "\n",
    "La función:\n",
    "\n",
    "```python\n",
    "build_tree(self, X, y, depth)\n",
    "```\n",
    "\n",
    "implementa el algoritmo recursivo para ID3 y CART:\n",
    "\n",
    "1. Crear un nodo con la clase mayoritaria.\n",
    "2. Comprobar condiciones de parada:\n",
    "    - nodo puro ,\n",
    "    - profundidad máxima alcanzada (pre-poda),\n",
    "    - tamaño mínimo insuficiente (pre-poda).\n",
    "3. Buscar el mejor split mediante `best_split`.\n",
    "    \\item Si existe un split válido:\n",
    "    - asignar atributo y umbral al nodo,\n",
    "    - dividir los datos en hijos izquierdo y derecho,\n",
    "    - llamar recursivamente al método para construir subárboles.\n",
    "\n",
    "--- \n",
    "### Predicción\n",
    "\n",
    "La predicción se realiza mediante un recorrido desde la raíz hasta una hoja:\n",
    "\n",
    "- En cada nodo interno se evalúa la condición:\n",
    "    $$\n",
    "        x[\\text{feature}] \\le \\text{threshold}.\n",
    "    $$\n",
    "- Se sigue la rama correspondiente.\n",
    "- Al alcanzar un nodo hoja, se devuelve la clase almacenada.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58d909dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myDecisionTreeClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, criterio=\"gini\", max_depth=None, min_samples_split=2):\n",
    "        self.criterion = criterio\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.tree_ = None # Almacenará el árbol de decisión entrenado\n",
    "\n",
    "    # Criterios\n",
    "\n",
    "    # Gini\n",
    "    def gini(self, y):\n",
    "        _, counts = np.unique(y, return_counts=True)\n",
    "        p = counts / counts.sum()\n",
    "        return 1.0 - np.sum(p**2)\n",
    "\n",
    "    # ID3\n",
    "    def entropy(self, y):\n",
    "        _, counts = np.unique(y, return_counts=True)\n",
    "        p = counts / counts.sum()\n",
    "        return -np.sum(p * np.log2(p + 1e-12)) # Se añade un valor pequeño para evitar el log(0)\n",
    "\n",
    "    def information_gain(self, y, y_left, y_right, impurity_func):\n",
    "        # Seleccionar el atributo que produzca la mayor reducción de la entropía de la clase después de la partición\n",
    "        impurity_parent = impurity_func(y)\n",
    "        n = len(y)\n",
    "        n_left, n_right = len(y_left), len(y_right)\n",
    "        \n",
    "        if n_left == 0 or n_right == 0:\n",
    "            return 0.0\n",
    "\n",
    "        impurity_children = (n_left / n) * impurity_func(y_left) + \\\n",
    "                            (n_right / n) * impurity_func(y_right)\n",
    "\n",
    "        gain = impurity_parent - impurity_children\n",
    "        return gain\n",
    "\n",
    "    def best_split(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        if n_samples < self.min_samples_split: # No dividir si no hay más del mínimo de muestras\n",
    "            return None, None\n",
    "        \n",
    "        # Elige la función de impureza\n",
    "        if self.criterion.lower() == \"id3\":\n",
    "            impurity_func = self.entropy\n",
    "        elif self.criterion.lower() == \"gini\":\n",
    "            impurity_func = self.gini\n",
    "        \n",
    "        best_gain = -np.inf\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "        \n",
    "        for feature_idx in range(n_features):\n",
    "            # valores ordenados de la característica\n",
    "            x_column = X[:, feature_idx]\n",
    "            # valores candidatos (puntos medios entre valores únicos)\n",
    "            unique_vals = np.unique(x_column)\n",
    "            if len(unique_vals) == 1:\n",
    "                continue  # no se puede dividir si todo es igual\n",
    "            \n",
    "            # candidatos tipo sklearn: puntos medios\n",
    "            thresholds = (unique_vals[:-1] + unique_vals[1:]) / 2.0\n",
    "            \n",
    "            for threshold in thresholds:\n",
    "                left_mask = x_column <= threshold\n",
    "                right_mask = ~left_mask\n",
    "                \n",
    "                y_left = y[left_mask]\n",
    "                y_right = y[right_mask]\n",
    "                \n",
    "                gain = self.information_gain(y, y_left, y_right, impurity_func)\n",
    "                \n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_feature = feature_idx\n",
    "                    best_threshold = threshold\n",
    "        \n",
    "        if best_gain <= 0:\n",
    "            return None, None\n",
    "        \n",
    "        return best_feature, best_threshold\n",
    "\n",
    "    # Construcción del árbol\n",
    "    def build_tree(self, X, y, depth):\n",
    "        # Nodo hoja si: puro, max_depth alcanzada o muy pocas muestras\n",
    "        num_samples = len(y)\n",
    "        unique_classes, counts = np.unique(y, return_counts=True)\n",
    "        majority_class = unique_classes[np.argmax(counts)]\n",
    "        \n",
    "        node = {\n",
    "            \"is_leaf\": False,\n",
    "            \"prediction\": majority_class,\n",
    "            \"feature_index\": None,\n",
    "            \"threshold\": None,\n",
    "            \"left\": None,\n",
    "            \"right\": None\n",
    "        }\n",
    "        \n",
    "        # criterios de parada\n",
    "        if len(unique_classes) == 1:  # todas las etiquetas iguales\n",
    "            node[\"is_leaf\"] = True\n",
    "            return node\n",
    "        \n",
    "        if self.max_depth is not None and depth >= self.max_depth:\n",
    "            node[\"is_leaf\"] = True\n",
    "            return node\n",
    "        \n",
    "        if num_samples < self.min_samples_split:\n",
    "            node[\"is_leaf\"] = True\n",
    "            return node\n",
    "        \n",
    "        # buscar mejor split\n",
    "        feature_idx, threshold = self.best_split(X, y)\n",
    "        \n",
    "        if feature_idx is None:  # no mejora\n",
    "            node[\"is_leaf\"] = True\n",
    "            return node\n",
    "        \n",
    "        # crear hijos\n",
    "        left_mask = X[:, feature_idx] <= threshold\n",
    "        right_mask = ~left_mask\n",
    "        \n",
    "        X_left, y_left = X[left_mask], y[left_mask]\n",
    "        X_right, y_right = X[right_mask], y[right_mask]\n",
    "        \n",
    "        node[\"feature_index\"] = feature_idx\n",
    "        node[\"threshold\"] = threshold\n",
    "        node[\"left\"] = self.build_tree(X_left, y_left, depth + 1)\n",
    "        node[\"right\"] = self.build_tree(X_right, y_right, depth + 1)\n",
    "        \n",
    "        return node\n",
    "\n",
    "    # Funciones fit() y predict()\n",
    "    def fit(self, X, y):\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "        self.tree_ = self.build_tree(X, y, depth=0)\n",
    "        return self\n",
    "    \n",
    "    def predict_one(self, x, node):\n",
    "        if node[\"is_leaf\"]:\n",
    "            return node[\"prediction\"]\n",
    "        \n",
    "        if x[node[\"feature_index\"]] <= node[\"threshold\"]:\n",
    "            return self.predict_one(x, node[\"left\"])\n",
    "        else:\n",
    "            return self.predict_one(x, node[\"right\"])\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = np.asarray(X)\n",
    "        return np.array([self.predict_one(x, self.tree_) for x in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbbd4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/data_clasificacion.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e913803b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4505445654589909\n",
      "F1 weighted: 0.4194611092861488\n",
      "Matriz de confusión:\n",
      " [[ 93   0   0  13  19   0   0  14   0  16   1]\n",
      " [  0   8   4   0   0  17  48   0   8  49 209]\n",
      " [  0   2  57   0   0  11  31   0   2  34 181]\n",
      " [ 21   0   0  57   4   0   0   8   0   7   3]\n",
      " [ 18   0   0   1  44   0   0   1   0  29   4]\n",
      " [  0   3   8   0   0 222  19   0   0  72  38]\n",
      " [  0  14  10   0   0  35  97   0  30 115 346]\n",
      " [ 19   0   0  13   2   0   0 110   0   0   0]\n",
      " [  0   5   1   0   0   3  10   0 230   2 213]\n",
      " [ 13   5  12   2  17  51  42   3   4 264 218]\n",
      " [ 11  10  26   6   5  21  89   2 126  96 845]]\n"
     ]
    }
   ],
   "source": [
    "X = df.drop(columns=[\"Class\"]).values\n",
    "y = df[\"Class\"].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "model = myDecisionTreeClassifier(\n",
    "    criterio=\"gini\",\n",
    "    max_depth=10,\n",
    "    min_samples_split=2\n",
    ")\n",
    "\n",
    "model.fit(X_train_scaled, y_train)\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"F1 weighted:\", f1_score(y_test, y_pred, average='weighted'))\n",
    "print(\"Matriz de confusión:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
