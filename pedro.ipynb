{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecb19a2b",
   "metadata": {},
   "source": [
    "# Notebook de Pedro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a443bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc5898c",
   "metadata": {},
   "source": [
    "# Clase `myLinearRegression`\n",
    "\n",
    "La clase implementa tres variantes del modelo de **regresión lineal**:\n",
    "**OLS**, **Ridge**, y **AutoRidge**.\n",
    "Todas comparten la misma estructura básica: estimar los coeficientes $\\beta$ que mejor explican $y \\approx X\\beta + \\beta_0$.\n",
    "\n",
    "---\n",
    "\n",
    "* **1. OLS (Ordinary Least Squares)**\n",
    "\n",
    "**Objetivo:**\n",
    "$$\n",
    "\\min_{\\beta_0,\\beta}||y - (\\beta_0 + X\\beta)||_2^2\n",
    "$$\n",
    "\n",
    "**Solución cerrada:**\n",
    "$$\n",
    "\\hat{\\theta} =\n",
    "\\begin{bmatrix}\\hat{\\beta}_0,\\hat{\\beta}\\end{bmatrix}^T\n",
    "(X^\\top X)^{-1} X^\\top y\n",
    "$$\n",
    "\n",
    "**Características algorítmicas:**\n",
    "\n",
    "* Calcula la solución exacta mediante la ecuación normal.\n",
    "* Sin ningún tipo de regularización.\n",
    "* Puede ser inestable si las variables están muy correlacionadas (matriz (X^\\top X) casi singular).\n",
    "\n",
    "---\n",
    "\n",
    "* **2. Ridge Regression (Regularización L2)**\n",
    "\n",
    "**Objetivo:**\n",
    "$$\n",
    "\\min_{\\beta_0,\\beta}; ||y - (\\beta_0 + X\\beta)||_2^2+\\alpha ||\\beta||_2^2\n",
    "$$\n",
    "\n",
    "**Solución analítica:**\n",
    "$$\n",
    "\\hat{\\theta}_{ridge} =\n",
    "(X^\\top X + \\alpha D)^{-1} X^\\top y\n",
    "$$\n",
    "donde $D = \\mathrm{diag}(0,1,\\dots,1)$ para **no penalizar el intercepto**.\n",
    "\n",
    "**Características algorítmicas:**\n",
    "\n",
    "* Introduce un término de penalización proporcional al cuadrado de los coeficientes.\n",
    "* Atenúa la varianza y mejora la estabilidad numérica.\n",
    "* El parámetro $\\alpha$ controla el grado de encogimiento de los coeficientes.\n",
    "\n",
    "---\n",
    "\n",
    "* **3. AutoRidge (Regularización adaptativa y validación interna)**\n",
    "\n",
    "**Objetivo:** aplicar una **penalización variable por característica** y encontrar de forma **automática** la intensidad óptima de regularización.\n",
    "\n",
    "**Etapas algorítmicas**\n",
    "\n",
    "1. **Penalización adaptativa por varianza**\n",
    "   Cada coeficiente recibe un peso proporcional a la varianza de su variable:\n",
    "   $$\n",
    "   \\lambda_j = \\frac{\\mathrm{Var}(X_j)}{\\overline{\\mathrm{Var}}}\n",
    "   $$\n",
    "   De esta forma, variables más dispersas se penalizan más.\n",
    "\n",
    "2. **Rejilla de factores de regularización**\n",
    "   Se prueban varios multiplicadores (k) alrededor de 1:\n",
    "   $$\n",
    "   k \\in \\{1-1.5\\gamma, 1-\\gamma, 1, 1+\\gamma, 1+1.5\\gamma\\}\n",
    "   $$\n",
    "   donde $\\gamma$ controla la amplitud de la búsqueda.\n",
    "\n",
    "3. **Evaluación Leave-One-Out (LOO)**\n",
    "   Para cada $k$, se calcula la matriz:\n",
    "   $$\n",
    "   A = X^\\top X + kD\n",
    "   \\quad\\text{y}\\quad\n",
    "   \\hat{y} = X A^{-1} X^\\top y\n",
    "   $$\n",
    "   Se estima el **error LOO** mediante:\n",
    "   $$\n",
    "   e_i^{LOO} = \\frac{y_i - \\hat{y}_i}{1 - h_{ii}},\n",
    "   \\qquad\n",
    "   MSE_{LOO}(k) = \\frac{1}{n}\\sum_i (e_i^{LOO})^2\n",
    "   $$\n",
    "   El $k$ que minimiza $MSE_{LOO}$ se selecciona como óptimo.\n",
    "\n",
    "4. **Reentrenamiento final**\n",
    "   Se calcula el modelo definitivo con el mejor (k^*):\n",
    "   $$\n",
    "   \\hat{\\theta}_{final} = (X^\\top X + k^* D)^{-1} X^\\top y\n",
    "   $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79da61f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myLinearRegression(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, method=\"ols\", alpha=1.0, gamma=0.1):\n",
    "        self.method   = method\n",
    "        self.alpha    = alpha\n",
    "        self.gamma    = gamma\n",
    "        self.coef_ = None\n",
    "        self.intercept_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        y = np.asarray(y, dtype=float).reshape(-1, 1)\n",
    "\n",
    "        if self.method == 'ols':\n",
    "            self._fit_ols_(X, y)\n",
    "        elif self.method == 'ridge':\n",
    "            self._fit_ridge_(X, y)\n",
    "        elif self.method == 'autoridge':\n",
    "            self._fit_autoridge_(X, y)\n",
    "        else:\n",
    "            raise ValueError(f\"Método desconocido: {self.method}\")\n",
    "\n",
    "        return self \n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        return X @ self.coef_ + self.intercept_\n",
    "\n",
    "\n",
    "    def _fit_ols_(self, X, y):\n",
    "        X_ = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "        XtX = X_.T @ X_\n",
    "        Xty = X_.T @ y\n",
    "        beta = np.inv(XtX) @ Xty\n",
    "        self.intercept_ = float(beta[0, 0])\n",
    "        self.coef_ = beta[1:, 0]\n",
    "\n",
    "    def _fit_ridge_(self, X, y):\n",
    "        X_ = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "        XtX = X_.T @ X_\n",
    "        Xty = X_.T @ y\n",
    "        D = np.eye(X_.shape[1]); D[0, 0] = 0.0  # no penalizar intercepto\n",
    "        beta = np.inv(XtX + self.alpha * D) @ Xty\n",
    "        self.intercept_ = float(beta[0, 0])\n",
    "        self.coef_ = beta[1:, 0]\n",
    "\n",
    "    def _fit_autoridge_(self, X, y):\n",
    "        # 1) penalización por varianza (intercepto no penalizado)\n",
    "        n, p = X.shape\n",
    "        X_  = np.hstack([np.ones((n, 1)), X])\n",
    "        XtX = X_.T @ X_\n",
    "        Xty = X_.T @ y\n",
    "\n",
    "        var = X.var(axis=0) + 1e-12                 # (p,)\n",
    "        base_lambdas = var / var.mean()             # media = 1\n",
    "        D_base = np.diag(np.hstack(([0.0], base_lambdas)))  # (p+1,p+1)\n",
    "\n",
    "        # 2) rejilla dependiendo de gamma\n",
    "        k_grid = np.array([1 - 1.5*self.gamma, 1 - self.gamma,1, 1 + self.gamma, 1 + 1.5*self.gamma])\n",
    "\n",
    "        best_k = None\n",
    "        best_mse = np.inf\n",
    "\n",
    "        for k in k_grid:\n",
    "            A = XtX + k * D_base\n",
    "            invA = np.linalg.inv(A)\n",
    "            beta = invA @ Xty\n",
    "            y_hat = X_ @ beta\n",
    "\n",
    "            # 3) MSE LOO (PRESS): e_loo = (y - y_hat) / (1 - h_ii)\n",
    "            H_diag = np.sum((X_ @ invA) * X_, axis=1)  # diag(X invA X^T)\n",
    "            resid = (y - y_hat).ravel()\n",
    "            e_loo = resid / (1.0 - H_diag + 1e-12)\n",
    "            mse_loo = float(np.mean(e_loo**2))\n",
    "\n",
    "            if mse_loo < best_mse:\n",
    "                best_mse = mse_loo\n",
    "                best_k = k\n",
    "        # refit con el mejor k\n",
    "        A = XtX + best_k * D_base\n",
    "        beta = np.linalg.inv(A) @ Xty\n",
    "        self.intercept_ = float(beta[0, 0])\n",
    "        self.coef_ = beta[1:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "23c286c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 283.87722567896213\n",
      "Mean Squared Error (Ridge): 283.87730541113467\n",
      "Mean Squared Error (AutoRidge): 283.87757178114214\n"
     ]
    }
   ],
   "source": [
    "datos = pd.read_csv('data/data_regresion.csv')\n",
    "\n",
    "# Dividimos los datos en conjunto de entrenamiento y prueba\n",
    "X = datos.drop('Popularity', axis=1)\n",
    "y = datos['Popularity']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "lr = myLinearRegression(method=\"ols\")\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred_lr = lr.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred_lr)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "lr2 = myLinearRegression(method=\"ridge\", alpha=0.3)\n",
    "lr2.fit(X_train, y_train)\n",
    "y_pred_lr2 = lr2.predict(X_test)\n",
    "mse2 = mean_squared_error(y_test, y_pred_lr2)\n",
    "print(\"Mean Squared Error (Ridge):\", mse2)\n",
    "\n",
    "l3 = myLinearRegression(method=\"autoridge\", gamma=0.2)\n",
    "l3.fit(X_train, y_train)\n",
    "y_pred_lr3 = l3.predict(X_test)\n",
    "mse3 = mean_squared_error(y_test, y_pred_lr3)\n",
    "print(\"Mean Squared Error (AutoRidge):\", mse3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
