{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecb19a2b",
   "metadata": {},
   "source": [
    "# Notebook de Pedro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4a443bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b03e2862",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Metrics_Regression:\n",
    "    # Diccionario a nivel de clase para almacenar todos los resultados de todas las ejecuciones\n",
    "    results = {}\n",
    "\n",
    "    def __init__(self, y_true, y_pred, validation_name, eps=1e-12):\n",
    "        # Aseguramos arrays 1D float para evitar problemas de tipos/Series\n",
    "        self.y_true = np.asarray(y_true, dtype=float).ravel()\n",
    "        self.y_pred = np.asarray(y_pred, dtype=float).ravel()\n",
    "        self.validation_name = validation_name\n",
    "        self.eps = float(eps)\n",
    "\n",
    "    # --- Métricas de regresión ---\n",
    "    def mse(self):\n",
    "        return mean_squared_error(self.y_true, self.y_pred)\n",
    "\n",
    "    def mae(self):\n",
    "        return mean_absolute_error(self.y_true, self.y_pred)\n",
    "\n",
    "    def rmse(self):\n",
    "        return np.sqrt(self.mse())\n",
    "\n",
    "    def r2(self):\n",
    "        return r2_score(self.y_true, self.y_pred)\n",
    "\n",
    "    def mape(self):\n",
    "        # *100 para porcentaje; eps evita divisiones por cero\n",
    "        return np.mean(np.abs((self.y_true - self.y_pred) / (np.abs(self.y_true) + self.eps))) * 100.0\n",
    "\n",
    "    def rmspe(self):\n",
    "        return np.sqrt(np.mean(((self.y_true - self.y_pred) / (np.abs(self.y_true) + self.eps)) ** 2)) * 100.0\n",
    "\n",
    "    # Para obtener todas las métricas de una vez, imprimirlas y almacenarlas\n",
    "    def all_metrics(self):\n",
    "        calculated_metrics = {\n",
    "            'MSE': self.mse(),\n",
    "            'MAE': self.mae(),\n",
    "            'RMSE': self.rmse(),\n",
    "            'R2': self.r2(),\n",
    "            'MAPE': self.mape(),\n",
    "            'RMSPE': self.rmspe()\n",
    "        }\n",
    "\n",
    "        # Guardar resultados\n",
    "        Metrics_Regression.results[self.validation_name] = calculated_metrics\n",
    "\n",
    "        # Imprimir con formato\n",
    "        print(f\"\\n===== Resultados {self.validation_name} =====\")\n",
    "        for k, v in calculated_metrics.items():\n",
    "            if k in ('MAPE', 'RMSPE'):\n",
    "                print(f'{k}: {v:.4f}%')\n",
    "            else:\n",
    "                print(f'{k}: {v:.6f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc5898c",
   "metadata": {},
   "source": [
    "# Clase `myLinearRegression`\n",
    "\n",
    "La clase implementa tres variantes del modelo de **regresión lineal**:\n",
    "**OLS**, **Ridge**, y **AutoRidge**.\n",
    "Todas comparten la misma estructura básica: estimar los coeficientes $\\beta$ que mejor explican $y \\approx X\\beta + \\beta_0$.\n",
    "\n",
    "---\n",
    "\n",
    "* **1. OLS (Ordinary Least Squares)**\n",
    "\n",
    "**Objetivo:**\n",
    "$$\n",
    "\\min_{\\beta_0,\\beta}||y - (\\beta_0 + X\\beta)||_2^2\n",
    "$$\n",
    "\n",
    "**Solución cerrada:**\n",
    "$$\n",
    "\\hat{\\theta} =\n",
    "\\begin{bmatrix}\\hat{\\beta}_0,\\hat{\\beta}\\end{bmatrix}^T\n",
    "(X^\\top X)^{-1} X^\\top y\n",
    "$$\n",
    "\n",
    "**Características algorítmicas:**\n",
    "\n",
    "* Calcula la solución exacta mediante la ecuación normal.\n",
    "* Sin ningún tipo de regularización.\n",
    "* Puede ser inestable si las variables están muy correlacionadas (matriz (X^\\top X) casi singular).\n",
    "\n",
    "---\n",
    "\n",
    "* **2. Ridge Regression (Regularización L2)**\n",
    "\n",
    "**Objetivo:**\n",
    "$$\n",
    "\\min_{\\beta_0,\\beta}; ||y - (\\beta_0 + X\\beta)||_2^2+\\alpha ||\\beta||_2^2\n",
    "$$\n",
    "\n",
    "**Solución analítica:**\n",
    "$$\n",
    "\\hat{\\theta}_{ridge} =\n",
    "(X^\\top X + \\alpha D)^{-1} X^\\top y\n",
    "$$\n",
    "donde $D = \\mathrm{diag}(0,1,\\dots,1)$ para **no penalizar el intercepto**.\n",
    "\n",
    "**Características algorítmicas:**\n",
    "\n",
    "* Introduce un término de penalización proporcional al cuadrado de los coeficientes.\n",
    "* Atenúa la varianza y mejora la estabilidad numérica.\n",
    "* El parámetro $\\alpha$ controla el grado de encogimiento de los coeficientes.\n",
    "\n",
    "---\n",
    "\n",
    "* **3. AutoRidge (Regularización adaptativa y validación interna)**\n",
    "\n",
    "**Objetivo:** aplicar una **penalización variable por característica** y encontrar de forma **automática** la intensidad óptima de regularización.\n",
    "\n",
    "**Etapas algorítmicas**\n",
    "\n",
    "1. **Penalización adaptativa por varianza**\n",
    "   Cada coeficiente recibe un peso proporcional a la varianza de su variable:\n",
    "   $$\n",
    "   \\lambda_j = \\frac{\\mathrm{Var}(X_j)}{\\overline{\\mathrm{Var}}}\n",
    "   $$\n",
    "   De esta forma, variables más dispersas se penalizan más.\n",
    "\n",
    "2. **Rejilla de factores de regularización**\n",
    "   Se prueban varios multiplicadores (k) alrededor de 1:\n",
    "   $$\n",
    "   k \\in \\{1-1.5\\gamma, 1-\\gamma, 1, 1+\\gamma, 1+1.5\\gamma\\}\n",
    "   $$\n",
    "   donde $\\gamma$ controla la amplitud de la búsqueda.\n",
    "\n",
    "3. **Evaluación Leave-One-Out (LOO)**\n",
    "   Para cada $k$, se calcula la matriz:\n",
    "   $$\n",
    "   A = X^\\top X + kD\n",
    "   \\quad\\text{y}\\quad\n",
    "   \\hat{y} = X A^{-1} X^\\top y\n",
    "   $$\n",
    "   Se estima el **error LOO** mediante:\n",
    "   $$\n",
    "   e_i^{LOO} = \\frac{y_i - \\hat{y}_i}{1 - h_{ii}},\n",
    "   \\qquad\n",
    "   MSE_{LOO}(k) = \\frac{1}{n}\\sum_i (e_i^{LOO})^2\n",
    "   $$\n",
    "   El $k$ que minimiza $MSE_{LOO}$ se selecciona como óptimo.\n",
    "\n",
    "4. **Reentrenamiento final**\n",
    "   Se calcula el modelo definitivo con el mejor (k^*):\n",
    "   $$\n",
    "   \\hat{\\theta}_{final} = (X^\\top X + k^* D)^{-1} X^\\top y\n",
    "   $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79da61f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myLinearRegression(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, method=\"ols\", alpha=1.0, gamma=0.1):\n",
    "        self.method   = method\n",
    "        self.alpha    = alpha\n",
    "        self.gamma    = gamma\n",
    "        self.coef_ = None\n",
    "        self.intercept_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        y = np.asarray(y, dtype=float).reshape(-1, 1)\n",
    "\n",
    "        if self.method == 'ols':\n",
    "            self._fit_ols_(X, y)\n",
    "        elif self.method == 'ridge':\n",
    "            self._fit_ridge_(X, y)\n",
    "        elif self.method == 'autoridge':\n",
    "            self._fit_autoridge_(X, y)\n",
    "        else:\n",
    "            raise ValueError(f\"Método desconocido: {self.method}\")\n",
    "\n",
    "        return self \n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        return X @ self.coef_ + self.intercept_\n",
    "\n",
    "\n",
    "    def _fit_ols_(self, X, y):\n",
    "        X_ = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "        XtX = X_.T @ X_\n",
    "        Xty = X_.T @ y\n",
    "        beta = np.linalg.inv(XtX) @ Xty\n",
    "        self.intercept_ = float(beta[0, 0])\n",
    "        self.coef_ = beta[1:, 0]\n",
    "\n",
    "    def _fit_ridge_(self, X, y):\n",
    "        X_ = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "        XtX = X_.T @ X_\n",
    "        Xty = X_.T @ y\n",
    "        D = np.eye(X_.shape[1]); D[0, 0] = 0.0  # no penalizar intercepto\n",
    "        beta = np.linalg.inv(XtX + self.alpha * D) @ Xty\n",
    "        self.intercept_ = float(beta[0, 0])\n",
    "        self.coef_ = beta[1:, 0]\n",
    "\n",
    "    def _fit_autoridge_(self, X, y):\n",
    "        # 1) penalización por varianza (intercepto no penalizado)\n",
    "        n, p = X.shape\n",
    "        X_  = np.hstack([np.ones((n, 1)), X])\n",
    "        XtX = X_.T @ X_\n",
    "        Xty = X_.T @ y\n",
    "\n",
    "        var = X.var(axis=0) + 1e-12                 # (p,)\n",
    "        base_lambdas = var / var.mean()             # media = 1\n",
    "        D_base = np.diag(np.hstack(([0.0], base_lambdas)))  # (p+1,p+1)\n",
    "\n",
    "        # 2) rejilla dependiendo de gamma\n",
    "        k_grid = np.array([1 - 1.5*self.gamma, 1 - self.gamma,1, 1 + self.gamma, 1 + 1.5*self.gamma])\n",
    "\n",
    "        best_k = None\n",
    "        best_mse = np.inf\n",
    "\n",
    "        for k in k_grid:\n",
    "            A = XtX + k * D_base\n",
    "            invA = np.linalg.inv(A)\n",
    "            beta = invA @ Xty\n",
    "            y_hat = X_ @ beta\n",
    "\n",
    "            # 3) MSE LOO (PRESS): e_loo = (y - y_hat) / (1 - h_ii)\n",
    "            H_diag = np.sum((X_ @ invA) * X_, axis=1)  # diag(X invA X^T)\n",
    "            resid = (y - y_hat).ravel()\n",
    "            e_loo = resid / (1.0 - H_diag + 1e-12)\n",
    "            mse_loo = float(np.mean(e_loo**2))\n",
    "\n",
    "            if mse_loo < best_mse:\n",
    "                best_mse = mse_loo\n",
    "                best_k = k\n",
    "        # refit con el mejor k\n",
    "        A = XtX + best_k * D_base\n",
    "        beta = np.linalg.inv(A) @ Xty\n",
    "        self.intercept_ = float(beta[0, 0])\n",
    "        self.coef_ = beta[1:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23c286c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "datos = pd.read_csv('data/data_regresion.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8eb46a",
   "metadata": {},
   "source": [
    "# 10 Holdout 75-25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82fd158d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividimos los datos en 10 conjuntos de entrenamiento y pruebas\n",
    "X = datos.drop('Popularity', axis=1)\n",
    "y = datos['Popularity']\n",
    "# 1. Inicializar listas para almacenar los 10 conjuntos\n",
    "X_train_list = []\n",
    "X_test_list = []\n",
    "y_train_list = []\n",
    "y_test_list = []\n",
    "\n",
    "# 2. Bucle para crear 10 divisiones y escalados diferentes\n",
    "NUM_SPLITS = 10\n",
    "\n",
    "for i in range(NUM_SPLITS):\n",
    "    # a. División: Usamos 'i' como random_state para asegurar 10 divisiones distintas\n",
    "    X_train_temp, X_test_temp, y_train_temp, y_test_temp = train_test_split(\n",
    "        X, y, \n",
    "        test_size=0.25, \n",
    "        random_state=i # CLAVE: Usar 'i' para tener 10 divisiones diferentes\n",
    "    )\n",
    "    \n",
    "    # b. Estandarización\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    #  fit_transform en Train\n",
    "    X_train_scaled = scaler.fit_transform(X_train_temp)\n",
    "    \n",
    "    # transform en Test (usando los parámetros aprendidos en Train)\n",
    "    X_test_scaled = scaler.transform(X_test_temp)\n",
    "    \n",
    "    # c. Almacenar los resultados en las listas\n",
    "    X_train_list.append(X_train_scaled)\n",
    "    X_test_list.append(X_test_scaled)\n",
    "    y_train_list.append(y_train_temp)\n",
    "    y_test_list.append(y_test_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08c36a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Resultados myLinearRegression Holdout 1 =====\n",
      "MSE: 288.519704\n",
      "MAE: 13.625248\n",
      "RMSE: 16.985868\n",
      "R2: 0.059034\n",
      "MAPE: 74.8760%\n",
      "RMSPE: 336.3711%\n",
      "\n",
      "===== Resultados myLinearRegression Holdout 2 =====\n",
      "MSE: 281.851256\n",
      "MAE: 13.382222\n",
      "RMSE: 16.788426\n",
      "R2: 0.064059\n",
      "MAPE: 67.7704%\n",
      "RMSPE: 316.7773%\n",
      "\n",
      "===== Resultados myLinearRegression Holdout 3 =====\n",
      "MSE: 288.169869\n",
      "MAE: 13.625127\n",
      "RMSE: 16.975567\n",
      "R2: 0.057240\n",
      "MAPE: 66.6737%\n",
      "RMSPE: 295.2974%\n",
      "\n",
      "===== Resultados myLinearRegression Holdout 4 =====\n",
      "MSE: 285.000948\n",
      "MAE: 13.481417\n",
      "RMSE: 16.881971\n",
      "R2: 0.049871\n",
      "MAPE: 65.8968%\n",
      "RMSPE: 280.1466%\n",
      "\n",
      "===== Resultados myLinearRegression Holdout 5 =====\n",
      "MSE: 288.118659\n",
      "MAE: 13.612079\n",
      "RMSE: 16.974058\n",
      "R2: 0.053591\n",
      "MAPE: 71.0002%\n",
      "RMSPE: 309.9980%\n",
      "\n",
      "===== Resultados myLinearRegression Holdout 6 =====\n",
      "MSE: 288.024679\n",
      "MAE: 13.535088\n",
      "RMSE: 16.971290\n",
      "R2: 0.058894\n",
      "MAPE: 74.6303%\n",
      "RMSPE: 323.7021%\n",
      "\n",
      "===== Resultados myLinearRegression Holdout 7 =====\n",
      "MSE: 286.002955\n",
      "MAE: 13.562946\n",
      "RMSE: 16.911622\n",
      "R2: 0.053914\n",
      "MAPE: 65.8628%\n",
      "RMSPE: 277.1891%\n",
      "\n",
      "===== Resultados myLinearRegression Holdout 8 =====\n",
      "MSE: 285.577134\n",
      "MAE: 13.567171\n",
      "RMSE: 16.899028\n",
      "R2: 0.053508\n",
      "MAPE: 67.3380%\n",
      "RMSPE: 291.4612%\n",
      "\n",
      "===== Resultados myLinearRegression Holdout 9 =====\n",
      "MSE: 291.023362\n",
      "MAE: 13.690366\n",
      "RMSE: 17.059407\n",
      "R2: 0.064588\n",
      "MAPE: 73.2217%\n",
      "RMSPE: 327.6251%\n",
      "\n",
      "===== Resultados myLinearRegression Holdout 10 =====\n",
      "MSE: 288.001979\n",
      "MAE: 13.555840\n",
      "RMSE: 16.970621\n",
      "R2: 0.050327\n",
      "MAPE: 73.7461%\n",
      "RMSPE: 309.3533%\n",
      "\n",
      "===== Métricas promedio myLinearRegression en 10 Holdouts =====\n",
      "MSE medio: 287.0291\n",
      "MAE medio: 13.5638\n",
      "RMSE medio: 16.9418\n",
      "R2 medio: 0.0565\n",
      "MAPE medio: 70.1016\n",
      "RMSPE medio: 306.7921\n"
     ]
    }
   ],
   "source": [
    "for i in range(NUM_SPLITS):\n",
    "    # a. Obtener los conjuntos de entrenamiento y prueba actuales\n",
    "    X_train_current = X_train_list[i]\n",
    "    X_test_current = X_test_list[i]\n",
    "    y_train_current = y_train_list[i]\n",
    "    y_test_current = y_test_list[i]\n",
    "    \n",
    "    # b. Entrenar el regresor myLinearRegression\n",
    "    lr = myLinearRegression(method='autoridge')\n",
    "    lr.fit(X_train_current, y_train_current)\n",
    "    y_pred_lr = lr.predict(X_test_current)\n",
    "    \n",
    "\n",
    "    # d. Metricas para el holdout actual\n",
    "    metrics_10holdout_lr = Metrics_Regression(y_test_current, y_pred_lr, f'myLinearRegression Holdout {i+1}')\n",
    "    metrics_10holdout_lr.all_metrics()\n",
    "\n",
    "\n",
    "# Calcular las métricas promedio para los 10 holdouts de myLinearRegression\n",
    "# Del diccionario results extraer las métricas y calcular la media para cada una\n",
    "# MSE\n",
    "all_mse = [Metrics_Regression.results[f'myLinearRegression Holdout {i+1}']['MSE'] for i in range(NUM_SPLITS)]\n",
    "mse_mean_10holdout = np.mean(all_mse)\n",
    "# MAE\n",
    "all_mae = [Metrics_Regression.results[f'myLinearRegression Holdout {i+1}']['MAE'] for i in range(NUM_SPLITS)]\n",
    "mae_mean_10holdout = np.mean(all_mae)\n",
    "# RMSE\n",
    "all_rmse = [Metrics_Regression.results[f'myLinearRegression Holdout {i+1}']['RMSE'] for i in range(NUM_SPLITS)]\n",
    "rmse_mean_10holdout = np.mean(all_rmse)\n",
    "# R2\n",
    "all_r2 = [Metrics_Regression.results[f'myLinearRegression Holdout {i+1}']['R2'] for i in range(NUM_SPLITS)]\n",
    "r2_mean_10holdout = np.mean(all_r2)\n",
    "# MAPE\n",
    "all_mape = [Metrics_Regression.results[f'myLinearRegression Holdout {i+1}']['MAPE'] for i in range(NUM_SPLITS)]\n",
    "mape_mean_10holdout = np.mean(all_mape)\n",
    "# RMSPE\n",
    "all_rmspe = [Metrics_Regression.results[f'myLinearRegression Holdout {i+1}']['RMSPE'] for i in range(NUM_SPLITS)]\n",
    "rmspe_mean_10holdout = np.mean(all_rmspe)\n",
    "\n",
    "\n",
    "# Imprimir las métricas promedio\n",
    "print('\\n===== Métricas promedio myLinearRegression en 10 Holdouts =====')\n",
    "print(f'MSE medio: {mse_mean_10holdout:.4f}')\n",
    "print(f'MAE medio: {mae_mean_10holdout:.4f}')\n",
    "print(f'RMSE medio: {rmse_mean_10holdout:.4f}')\n",
    "print(f'R2 medio: {r2_mean_10holdout:.4f}')\n",
    "print(f'MAPE medio: {mape_mean_10holdout:.4f}')\n",
    "print(f'RMSPE medio: {rmspe_mean_10holdout:.4f}')\n",
    "\n",
    "\n",
    "# 1. Crear el diccionario de resultados medios\n",
    "mean_results = {\n",
    "    'MSE':   mse_mean_10holdout,\n",
    "    'MAE':   mae_mean_10holdout,\n",
    "    'RMSE':  rmse_mean_10holdout,\n",
    "    'R2':    r2_mean_10holdout,\n",
    "    'MAPE':  mape_mean_10holdout,\n",
    "    'RMSPE': rmspe_mean_10holdout\n",
    "}\n",
    "\n",
    "# 2. Guardar en Metrics_Regression.results con una clave única\n",
    "Metrics_Regression.results['myLinearRegression_10_Holdouts_MEAN'] = mean_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
